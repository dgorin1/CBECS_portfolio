{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ac1ab3",
   "metadata": {},
   "source": [
    "# Here we'll train some models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd81c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "import pathlib, yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys, pathlib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, ElasticNetCV\n",
    "import warnings, pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "\n",
    "\n",
    "\n",
    "# --- project paths\n",
    "PROJ = pathlib.Path().resolve().parent\n",
    "CFG  = yaml.safe_load(open(PROJ / \"conf\" / \"config.yaml\"))\n",
    "var_types = yaml.safe_load(open(PROJ / \"conf\" / \"variable_types.yaml\"))\n",
    "RAW_DIR = PROJ / CFG[\"data\"][\"raw_dir\"]\n",
    "PROC_DIR = PROJ / CFG[\"data\"][\"processed_dir\"]\n",
    "numeric_cols = var_types[\"numeric_variables\"]\n",
    "RULES = yaml.safe_load(open(PROJ/\"conf\"/\"cleaning_rules.yaml\"))\n",
    "\n",
    "if str(PROJ) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJ))\n",
    "\n",
    "from pipeline.transformers import (MedianImputer, AutoTransform, OneHotEncoder) \n",
    "PerfWarn = pd.errors.PerformanceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=PerfWarn)\n",
    "\n",
    "# ---------- Column groups from config ----------\n",
    "TYPES = yaml.safe_load(open(PROJ/\"conf\"/\"variable_types.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0464e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split correctly: True\n"
     ]
    }
   ],
   "source": [
    "# First read in the file...\n",
    "df = pd.read_parquet(PROC_DIR / \"cbecs_2018_clean.parquet\")\n",
    "cat_cols = [c for c in TYPES[\"categorical_variables\"] if c in df.columns]\n",
    "num_cols = [c for c in TYPES[\"numeric_variables\"] if c in df.columns]\n",
    "\n",
    "\n",
    "# Assign X, y \n",
    "X = df.drop('LOG_MFBTU', axis=1)\n",
    "y = df['LOG_MFBTU']\n",
    "\n",
    "# Split the data into validation, train, and test sets\n",
    "# e.g., 80/10/10 split\n",
    "test_size = 0.10\n",
    "val_size  = 0.10\n",
    "rand      = 42\n",
    "\n",
    "# 1) Hold out test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=rand, shuffle=True\n",
    ")\n",
    "\n",
    "# 2) From the remainder, carve out validation\n",
    "val_rel = val_size / (1.0 - test_size)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=val_rel, random_state=rand, shuffle=True\n",
    "    )\n",
    "\n",
    "# Check and make sure the splits worked as expected...\n",
    "df.shape\n",
    "check_shape = X_train.shape[0]+X_test.shape[0]+X_val.shape[0] == df.shape[0]\n",
    "\n",
    "print(f\"Split correctly: {check_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea6cfd3",
   "metadata": {},
   "source": [
    "# Run Linear-specific pipeline to finish preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8162709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "base_steps = [\n",
    "    (\"median_imputer\", MedianImputer(RULES[\"impute_rules\"])),\n",
    "    (\"auto\", AutoTransform(scoring=\"r2\", cv=5, n_jobs=-1, num_cols=num_cols, suffix_identity=False)),\n",
    "    (\"ohe\", OneHotEncoder(var_types[\"categorical_variables\"], var_types[\"numeric_variables\"])),\n",
    "    (\"scaler\", StandardScaler(with_mean=False))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "913f67bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS: -826741400936559044198400.000\n",
      "Ridge: 0.917\n",
      "Lasso: 0.923\n",
      "ElasticNet: 0.923\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "\n",
    "# 1️⃣ Define and fit preprocessing pipeline once\n",
    "preproc = Pipeline(steps=[\n",
    "    (\"median_imputer\", MedianImputer(RULES[\"impute_rules\"])),\n",
    "    (\"auto\", AutoTransform(scoring=\"r2\", cv=5, n_jobs=-1, num_cols=num_cols, suffix_identity=False)),\n",
    "    (\"ohe\", OneHotEncoder(var_types[\"categorical_variables\"], var_types[\"numeric_variables\"])),\n",
    "    (\"scaler\", StandardScaler(with_mean=False))\n",
    "])\n",
    "\n",
    "X_train_proc = preproc.fit_transform(X_train, y_train)\n",
    "X_test_proc = preproc.transform(X_test)\n",
    "\n",
    "# 2️⃣ Try different linear models on the preprocessed data\n",
    "models = {\n",
    "    \"OLS\": LinearRegression(),\n",
    "    \"Ridge\": RidgeCV(alphas=np.logspace(-6, 6, 25), cv=5),\n",
    "    \"Lasso\": LassoCV(cv=5, n_jobs=-1),\n",
    "    \"ElasticNet\": ElasticNetCV(cv=5, n_jobs=-1)\n",
    "}\n",
    "\n",
    "for name, m in models.items():\n",
    "    score = cross_val_score(m, X_train_proc, y_train, cv=5, scoring=\"r2\").mean()\n",
    "    print(f\"{name}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ae8991",
   "metadata": {},
   "source": [
    "### Quick thoughts about the above:\n",
    "When I initially fit an unregularized OLS model, performance collapsed (R² ≈ -8×10²³). This indicates severe multicollinearity in the expanded feature space — a common issue after one-hot encoding. I therefore implemented Ridge and Lasso regularization, which stabilized the solution and improved cross-validated R² to ~0.92. I considered manually examining collinearity of features and pruning appropriately, but, given the width of this dataset, I opted for regularization instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3a0c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
